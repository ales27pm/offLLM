name: Convert Dolphin to Core ML

on:
  push:
    branches:
      - main

jobs:
  convert:
    runs-on: macos-latest
    environment: offLLM
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install torch==2.2.0 transformers==4.44.2 coremltools==7.2 numpy==1.26.4 huggingface-hub==0.25.1

      - name: Convert model to Core ML
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python - <<'EOF'
          import os
          import warnings
          from typing import Any, Optional, Sequence
          from huggingface_hub import login
          import torch
          from transformers import LlamaForCausalLM, AutoConfig
          from transformers.cache_utils import Cache
          import coremltools as ct
          import numpy as np

          # Suppress deprecation warning
          warnings.filterwarnings("ignore", category=FutureWarning)

          # Verify token
          token = os.getenv("HF_TOKEN")
          if not token:
              print("Error: HF_TOKEN environment variable is not set.")
              exit(1)

          # Log in to Hugging Face
          try:
              login(token=token)
              print("Successfully logged in to Hugging Face")
          except Exception as e:
              print(f"Failed to log in to Hugging Face: {e}")
              exit(1)

          class SliceUpdateKeyValueCache(Cache):
              def __init__(self, *, shape: Sequence[int], dtype: torch.dtype = torch.float32) -> None:
                  super().__init__()
                  self.past_seen_tokens: int = 0
                  self.k: torch.Tensor = torch.zeros(shape, dtype=dtype)
                  self.v: torch.Tensor = torch.zeros(shape, dtype=dtype)

              def update(self, k_state: torch.Tensor, v_state: torch.Tensor, layer_idx: int, cache_kwargs: Optional[dict[str, Any]] = None) -> tuple[torch.Tensor, torch.Tensor]:
                  position = cache_kwargs.get("cache_position", None)
                  assert position is not None, "cache_position required to update cache."
                  begin, end = self.past_seen_tokens, self.past_seen_tokens + position.shape[-1]
                  self.k[layer_idx, :, : k_state.shape[1], begin:end, :] = k_state
                  self.v[layer_idx, :, : v_state.shape[1], begin:end, :] = v_state
                  k_state = self.k[layer_idx, :, :, :end, :]
                  v_state = self.v[layer_idx, :, :, :end, :]
                  return k_state, v_state

              def get_seq_length(self, _: int = 0) -> int:
                  return self.past_seen_tokens

          try:
              # Load model
              model_id = "cognitivecomputations/Dolphin3.0-Llama3.2-3B"
              config = AutoConfig.from_pretrained(model_id)
              torch_model = LlamaForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).eval()

              # Stateful wrapper with KV cache
              num_layers = len(torch_model.model.layers)
              num_heads = torch_model.config.num_attention_heads
              head_dim = torch_model.config.hidden_size // num_heads
              batch_size, context_size = 1, 2048
              kv_cache_shape = (num_layers, batch_size, num_heads, context_size, head_dim)

              class KvCacheStateLlamaForCausalLM(torch.nn.Module):
                  def __init__(self, model):
                      super().__init__()
                      self.model = model
                      self.kv_cache = SliceUpdateKeyValueCache(shape=kv_cache_shape, dtype=torch.float16)

                  @torch.no_grad()
                  def forward(self, input_ids: torch.LongTensor, attention_mask: torch.LongTensor, cache_position: torch.LongTensor) -> torch.Tensor:
                      out = self.model(
                          input_ids=input_ids,
                          attention_mask=attention_mask,
                          past_key_values=self.kv_cache,
                          cache_position=cache_position,
                          use_cache=True,
                      )
                      self.kv_cache.past_seen_tokens += input_ids.shape[-1]
                      return out.logits

              wrapper = KvCacheStateLlamaForCausalLM(torch_model)

              # Define input shapes (use example with seq_len=1 for extend phase)
              input_shape = (batch_size, 1)  # Dynamic for extend
              example_input_ids = torch.zeros(input_shape, dtype=torch.int64)
              example_attention_mask = torch.zeros(input_shape, dtype=torch.int64)
              example_cache_position = torch.tensor([0], dtype=torch.int64)

              # Trace the model
              traced_model = torch.jit.trace(wrapper, (example_input_ids, example_attention_mask, example_cache_position))

              # Define Core ML inputs and outputs with flexible shapes
              seq_dim = ct.RangeDim(lower_bound=1, upper_bound=context_size, default=1)
              inputs = [
                  ct.TensorType(name="input_ids", shape=(batch_size, seq_dim), dtype=np.int32),
                  ct.TensorType(name="attention_mask", shape=(batch_size, seq_dim), dtype=np.int32),
                  ct.TensorType(name="cache_position", shape=(seq_dim,), dtype=np.int32),
              ]
              outputs = [ct.TensorType(name="logits", dtype=np.float16)]

              # Convert to Core ML
              mlmodel = ct.convert(
                  traced_model,
                  inputs=inputs,
                  outputs=outputs,
                  minimum_deployment_target=ct.target.macOS15,  # Required for stateful
                  skip_model_load=True,
              )

              # Quantization
              quant_config = ct.optimize.coreml.OpLinearQuantizerConfig(
                  mode="linear_symmetric",
                  dtype=ct.optimize.coreml.data_types.int4,
                  granularity="per_block",
                  block_size=32,
              )
              quantized_mlmodel = ct.optimize.coreml.linear_quantize_weights(mlmodel, config=quant_config)

              # Save model
              quantized_mlmodel.save("Dolphin3.0-Llama3.2-3B.mlpackage")
              print("Model conversion successful")
          except Exception as e:
              print(f"Error during conversion: {e}")
              exit(1)
          EOF

      - name: Commit the converted model
        run: |
          git config user.name github-actions
          git config user.email github-actions@github.com
          git add Dolphin3.0-Llama3.2-3B.mlpackage
          git commit -m "Add converted Core ML model" || echo "No changes to commit"
          git push
        continue-on-error: true

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: coreml-model
          path: Dolphin3.0-Llama3.2-3B.mlpackage
